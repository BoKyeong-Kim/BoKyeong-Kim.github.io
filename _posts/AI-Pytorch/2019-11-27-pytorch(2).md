---
layout: post
title: Pytorch 기초(2)
subtitle: < 파이토치란? >
feature-img: "assets/img/sample_feature_img_2.png"
categories : [AI/Pytorch]
tags: [AI,ML,Pytorch,파이토치란]
---


**Pytorch** 기초 2탄!  

<br>

이번 게시글은 ***[파이토치(Pytorch) 3. nn & nn.functional](
https://www.youtube.com/watch?v=SMOuUKrhTro)***영상과 ***[PyTorch로 딥러닝하기: 60분만에 끝장내기](
https://9bow.github.io/PyTorch-tutorials-kr-0.3.1/beginner/blitz/neural_networks_tutorial.html)*** 블로그를 참고했습니다.

<br>

#### 신경망의 학습 과정
1) 학습 가능한 가중치(weight)를 갖는 신경망을 정의<br>
2) 데이터셋(dataset) 입력을 반복<br>
3) 입력을 신경망에서 처리<br>
4) 손실(loss) 즉, 정답으로부터 얼마나 떨어져있는지 계산<br>
5) 변화도(gradient)를 신경망의 매개변수들에 역으로 전파(역전파)<br>
6) 신경망의 가중치를 갱신(update). <br>

아래의 규칙을 일반적으로 사용.
`가중치(wiehgt) = 가중치(weight) - 학습율(learning rate) * 변화도(gradient)`
<br>

#### Pytorch

##### nn에서 제공하는 기능

* Parameters
* Containers
* Conv
* Pooing
* Padding
* Non-linear Activation
* Normalization
* Recurrent
* Linear
* Dropout
* Sparse
* Distance
* Loss
* Data Parallel
* Utilities

<br>

##### nn.functional 에서 제공하는 기능

* Conv
* Pooling
* Non-linear activation
* Normalization
* Linear function(=fully connected layer)
* Dropout
* Distance
* Loss
* Vision


두개의 기능이 비슷한데 약간의 차이가 있다고 한다.

<br>

### nn & nn.functional



#### nn.functional의 Conv사용해보기(F.conv2d)

* nn.functional 사용하려면 weight를 선언해줘야한다.
```python
import torch
import torch.nn
import torch.nn.functional as F 
from torch.autograd import Variable
```
<br>

*  여기서는 filter가 weight가 된다.
```python
>> input = torch.ones(1,1,3,3)
>> filter = torch.ones(1,1,3,3)
```

<br>

```python
>> input.shape
```
```
torch.Size([1, 1, 3, 3])
```

<br>

* requires_grad = True이기 때문에 backward 가능
* input Variable 선언

```python
>> input = Variable(input, requires_grad = True)
>> print(input)
```
```
tensor([[[[1., 1., 1.],
          [1., 1., 1.],
          [1., 1., 1.]]]], requires_grad=True)
```
<br>

* filter Variable 선언

```python
>> filter = Variable(filter)
```

* input과 filter를 conv2d에 넣어주면 out 했을때 원하는 결과를 얻을 수 있다.

```python
>> out = F.conv2d(input, filter)
>> print(out)
```

```
tensor([[[[9.]]]], grad_fn=<MkldnnConvolutionBackward>)
```

* out을 backward를 시켜주고 grad_fn을 확인하면 ConvolutionBackward 한다는 것을 확인할 수 있다.
* 역전파(backprop)를 out.backward()로 표현한다.

```python
>> out.backward
>> print(out.grad_fn)
```

```
<MkldnnConvolutionBackward object at 0x12574a910>
```


* 변화도 d(out)/dx를 출력
* gradient가 1인 것을 확인할 수 있다.
```python
>> print(input.grad)
```

```
tensor([[[[1., 1., 1.],
          [1., 1., 1.],
          [1., 1., 1.]]]])
```

<br>

#### nn의 Conv사용해보기


```python
import torch
import torch.nn as nn
from torch.autograd import Variable
```

* forward의 입력은 autograd.Variable 이고, 출력 또한 마찬가지

```python
>> input = torch.ones(1,1,3,3)
>> input = Variable(input, requires_grad=True)
>> print(input)
```

```
tensor([[[[1., 1., 1.],
          [1., 1., 1.],
          [1., 1., 1.]]]], requires_grad=True)
```

<br>

* Conv2d(input channels, output channels, 3x3 square convolution)
```python
>> func = nn.Conv2d(1,1,3)
>> func.weight
```

```
Parameter containing:
tensor([[[[-0.1980, -0.1202, -0.2932],
          [-0.1648,  0.1544,  0.1094],
          [-0.0429, -0.1280, -0.0726]]]], requires_grad=True)
```

<br>

* bias value를 가지고 있어서 bias가 추가되어야 아래의 값이 나온다.
```python
>> out = func(input)
>> print(out)
```

```
tensor([[[[-0.4534]]]], grad_fn=<MkldnnConvolutionBackward>)
```
<br>

```python
>> print(input.grad)
```

```
None
```
<br>

```python
>> out.backward()
```
<br>

* filter만 곱해줬기 때문에 filter 자체가 값이된다.

```python
>> print(input.grad)
```

```
tensor([[[[-0.1980, -0.1202, -0.2932],
          [-0.1648,  0.1544,  0.1094],
          [-0.0429, -0.1280, -0.0726]]]])
```


